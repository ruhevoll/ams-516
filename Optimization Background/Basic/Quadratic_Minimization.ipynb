{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8b5fc0-6d3a-4960-ba0b-5c98a1038783",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61bbb73-4857-402b-ab3e-e8055e8770d7",
   "metadata": {},
   "source": [
    "## 1.1 Quadratic Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db8149-b064-4544-9d5d-78281fd83818",
   "metadata": {},
   "source": [
    "- A general quadratic function of $n$ variables can be written $$f(x) = \\sum_{i = 1}^n c_i x_i + \\frac{1}{2} \\sum_{i = 1}^n \\sum_{j = 1}^n \\gamma_{ij} x_i x_j.$$\n",
    "- Let $\\mathbf{c} = (c_1, \\dots, c_n)^{\\mathsf T}$, $\\mathbf{x} = (x_1, \\dots, x_n)^{\\mathsf{T}}$, and $\\mathbf{C} = [\\gamma_{ij}]$, so that we attain the more compact form $$f(\\mathbf{x}) = \\mathbf{c}^{\\mathsf T} \\mathbf{x} + \\frac{1}{2} \\mathbf{x}^{\\mathsf{T}} \\mathbf{C} \\mathbf{x}. \\tag{1.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30cde4b-1711-44e9-967e-8453e77a458a",
   "metadata": {},
   "source": [
    "Here, we see that $f: \\mathbb{R}^n \\to \\mathbb{R}$ is a real-valued function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6300902-74b6-4231-a3e6-ba26c472e0d4",
   "metadata": {},
   "source": [
    "- **We assume $\\mathbf{C}$ is symmetric**, otherwise it may be replaced by $\\frac{1}{2}[\\mathbf{C} + \\mathbf{C}^{\\mathsf{T}}]$ which is.\n",
    "- **We assume $\\mathbf{C}$ is SPD**, (symmetric, positive semidefinite). Therefore, $\\mathbf{s}^{\\mathsf{T}} \\mathbf{C} \\mathbf{s} \\geq 0$ for all $\\mathbf{s} \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75477822-7f61-4d07-9094-7432baa10335",
   "metadata": {},
   "source": [
    "- The gradient of (1.1) with respect to the spatial vector $\\mathbf{x}$ is $$\\begin{align*} \\nabla f(\\mathbf{x}) &= \\nabla (\\mathbf{c}^{\\mathsf{T}}\\mathbf{x}) + \\frac{1}{2} \\nabla (\\mathbf{x}^{\\mathsf{T}} \\mathbf{C} \\mathbf{x}) \\\\ &= (\\mathbf{c} \\nabla_{\\mathbf{x}} \\mathbf{x} + \\mathbf{x}^{\\mathsf{T}} \\nabla_{\\mathbf{x}} \\mathbf{c}^{\\mathsf{T}}) + \\frac{1}{2} 2 \\mathbf{C} \\mathbf{x} \\\\ &= \\mathbf{c} + \\mathbf{C}\\mathbf{x}. \\tag{1.2}\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517a437-6204-455e-8c0c-1775a6357f80",
   "metadata": {},
   "source": [
    "> **Example:** Solve $$\\min \\{-4x_1 - 4x_2 + x_1^2 + x_2^2 \\mid x_1 + x_2 = 2\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2397845-910e-4590-b062-40de44cac644",
   "metadata": {},
   "source": [
    "In the form of (1.1), we have $$f(\\mathbf{x}) = \\begin{bmatrix} -4 & -4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} x_1 & x_2\\end{bmatrix} \\begin{bmatrix} 2 & 0 \\\\ 0 & 2\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}.$$ The Lagrangian is therefore $$\\mathcal{L}(\\mathbf{x}, \\lambda) = \\begin{bmatrix} -4 & -4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} x_1 & x_2\\end{bmatrix} \\begin{bmatrix} 2 & 0 \\\\ 0 & 2\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + \\lambda\\left(2 - \\begin{bmatrix} 1 & 1\\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790d31d-02fe-4884-ad97-9ee19c6316b1",
   "metadata": {},
   "source": [
    "Therefore, the total derivative of $\\mathcal{L}$ is $$\\nabla_{\\mathbf{x}, \\lambda} \\mathcal{L} = \\left\\{\\begin{bmatrix} -4 - \\lambda \\\\ -4 -\\lambda\\end{bmatrix} + \\begin{bmatrix} 2 & 0 \\\\ 0 & 2\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}, 2 - \\begin{bmatrix} 1 & 1\\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b21ad81-0477-4286-908a-d98fa123ff4f",
   "metadata": {},
   "source": [
    "$\\nabla_{\\mathbf{x}, \\lambda} \\mathcal{L} = 0$ is equivalent to the following system $$\\begin{bmatrix} 1 & 1 & 0 \\\\ 2 & 0 & -1 \\\\ 0 & 2 & -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\lambda \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 4\\end{bmatrix} \\implies \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\lambda \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\2\\end{bmatrix}.$$ So, the minimum is at $(x_1, x_2) = (1, 1)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca2771b-6899-45c2-955b-f1f1cafaafd4",
   "metadata": {},
   "source": [
    "We can also use `scipy` in Python to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389e84ad-cde4-4ffd-9833-11788e58fc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n",
      "-6.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize, LinearConstraint\n",
    "import numpy as np \n",
    "# To use scipy optimize, follow these steps:\n",
    "# 1. Define your objective function in terms of the variables x[0], x[1], ... x[n - 1]\n",
    "# 2. Define an initial guess x0 = [x0, x1, ..., xn-1]\n",
    "# 3. Define your linear constraint using LinearConstraint(b, lb, ub) where\n",
    "# if your constraint is b^T x = d, lb = ub = d \n",
    "# 4. minimize(objective_function, x0, constraints = [linear_constraint])\n",
    "\n",
    "def objective_function(x):\n",
    "    return x[0]**2 + x[1]**2 - 4*x[0] - 4*x[1]\n",
    "\n",
    "x0 = np.array([0.5, 0.5])\n",
    "b = np.array([1, 1])\n",
    "lb = 2\n",
    "ub = 2\n",
    "\n",
    "linear_constraint = LinearConstraint(b, lb, ub)\n",
    "\n",
    "result = minimize(objective_function, x0, constraints = [linear_constraint])\n",
    "# Optimal solution\n",
    "print(result.x)\n",
    "\n",
    "# Minimum objective value\n",
    "print(result.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004993f3-180b-4205-8c4d-04dfc0866f56",
   "metadata": {},
   "source": [
    "In general, when we consider the **model problem** $$\\min \\{\\mathbf{c}^{\\mathsf{T}} \\mathbf{x} + \\frac{1}{2} \\mathbf{x}^{\\mathsf{T}} \\mathbf{C} \\mathbf{x} \\mid \\mathbf{A}\\mathbf{x} = \\mathbf{b}\\} \\tag{1.4}$$ for $\\mathbf{c}, \\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{C} \\in \\mathbb{R}^{n \\times n}$, $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{b} \\in \\mathbb{R}^m$, the **optimality conditions** for (1.4) are \n",
    "1. $\\mathbf{A} \\mathbf{x}_0 = \\mathbf{b}$ and,\n",
    "2. there exists a vector $u$ with $-\\nabla f(\\mathbf{x}_0) = \\mathbf{A}^{\\mathsf{T}} \\mathbf{u}$, where $\\mathbf{u}$ is called the **multiplier vector** for the problem. It has one component for each row of $A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1141aad6-5604-4d08-9644-4be2beb442c4",
   "metadata": {},
   "source": [
    "**Theorem.** \n",
    "1. $\\mathbf{x}_0$ is optimal for (1.4) if and only if $\\mathbf{x}_0$ satisfies the optimality conditions for (1.4).\n",
    "2. $\\mathbf{x}_0$ is optimal for (1.4) if and only if there exists an $m$-vector $\\mathbf{u}$ such that $(\\mathbf{x}_0, \\mathbf{u})$ satisfies the linear equations $$\\begin{bmatrix} \\mathbf{C} & \\mathbf{A}^{\\mathsf{T}} \\\\ \\mathbf{A} & 0\\end{bmatrix} \\begin{bmatrix} \\mathbf{x}_0 \\\\ \\mathbf{u}\\end{bmatrix} = \\begin{bmatrix} - \\mathbf{c} \\\\ \\mathbf{b} \\end{bmatrix}.$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec53353-caeb-4478-9a07-5ecd281052e7",
   "metadata": {},
   "source": [
    "## 1.2 - Nonlinear Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b9c42-a404-45ef-8a44-3b2d2824a3ba",
   "metadata": {},
   "source": [
    "**Theorem.** Suppose:\n",
    "1. $\\mathbf{A}\\mathbf{x}_0 = \\mathbf{b}$,\n",
    "2. there exists an $m$-vector $u$ with $-\\nabla \\mathbf{x}(x_0) = \\mathbf{A}^{\\mathsf{T}} \\mathbf{u}$, and\n",
    "3. the **Hessian** $$H(\\mathbf{x}) = \\left[\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_i \\partial x_j}\\right]$$ evaluated at $\\mathbf{x} = \\mathbf{x}_0$ is positive definite.\n",
    "Then, $\\mathbf{x}_0$ is a local minimizer for $$\\min \\{f(\\mathbf{x}) \\mid \\mathbf{A}\\mathbf{x} = \\mathbf{b}\\}.$$ If, in addition, $H(\\mathbf{x})$ is positive definite for all $\\mathbf{x}$ such that $\\mathbf{Ax} = \\mathbf{b}$, then $\\mathbf{x}_0$ is a global minimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128de0c-5f95-4d0b-bea2-1abb9fecf74b",
   "metadata": {},
   "source": [
    "Note that, for a quadratic function, $H = \\mathbf{C}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed10ab3a-306f-42a8-8d95-19d8b1d07064",
   "metadata": {},
   "source": [
    "## 1.3 - Extreme Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8ab8e-9d2d-4aad-9260-2467068d47b3",
   "metadata": {},
   "source": [
    "A **linear programming** (LP) problem is the problem of minimizing a linear function subject to linear inequality/equality constraints. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4866fea-06b6-4e31-b6e0-7bde93b9d108",
   "metadata": {},
   "source": [
    "Consider the **feasible region** $R$ defined by $$R = \\{\\mathbf{x} \\mid \\mathbf{a}_i^{\\mathsf{T}} \\mathbf{x} \\leq b_i, \\qquad i = 1, 2, \\dots, m\\}$$ where $\\mathbf{a}_i$ is a given $n$-vector and each $b_i$ is a given scalar. Written more compactly, for $A = [\\mathbf{a}_1, \\mathbf{a}_2, \\dots, \\mathbf{a}_m]^{\\mathsf{T}}$, $$R = \\{\\mathbf{x} \\mid \\mathbf{Ax} \\leq \\mathbf{b}\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d538c3-18af-4883-add5-c64f6e453037",
   "metadata": {},
   "source": [
    "- A point $\\mathbf{x}_0$ is **feasible** if $\\mathbf{x}_0 \\in R$ and **infeasible** otherwise.\n",
    "- For any $i$ with $1 \\leq i \\leq m$, the constraint $i$ is **active** at $\\mathbf{x}_0$ if $\\mathbf{a}_i^{\\mathbf{T}} \\mathbf{x}_0 = b_i$ and **inactive** at $\\mathbf{x}_0$ if $\\mathbf{a}_i^{\\mathsf{T}}\\mathbf{x}_0 < b_i$.\n",
    "- A point $\\mathbf{x}_0$ is an **extreme point** of $R$ if $x_0 \\in R$ and there are $n$ constraints having linearly independent gradients active at $\\mathbf{x}_0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33281950-36c5-4df2-a2cf-b94dafbd99c0",
   "metadata": {},
   "source": [
    "**Theorem.** If $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, then the LP problem $R$ generated by $\\mathbf{A}$ has extreme points (supposing $R$ is nonempty) if and only if $\\mathrm{rank}(\\mathbf{A}) = n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8b4224-82e4-45a4-98a7-fe0c22b20765",
   "metadata": {},
   "source": [
    "In the case of the LP problem $$\\min \\{\\mathbf{c}^{\\mathsf{T}}\\mathbf{x} \\mid \\mathbf{A} \\mathbf{x} \\leq b\\},$$ we have the following theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c378a0e-57ed-480c-9710-beea6cdd8152",
   "metadata": {},
   "source": [
    "**Theorem.** For the LP problem above, assume that $\\mathrm{rank}(\\mathbf{A}) = n$ and that the problem is bounded from below. Then one of the extreme points of the above LP problem is an optimal solution for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9ae62d-4559-42bb-b177-731b497f784b",
   "metadata": {},
   "source": [
    "So, when $\\mathbf{A}$ has full rank and the problem is bounded from below, we can just look at the extreme points to find our optimal solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228a5dab-896a-4117-b699-dc8df9b530a4",
   "metadata": {},
   "source": [
    "> **Example:** Solve $$\\min \\{-\\boldsymbol{\\mu}^{\\mathsf{T}}\\mathbf{x} \\mid \\mathbf{x} \\geq 0, \\mathbf{l}^{\\mathsf{T}}\\mathbf{x} = 1\\}$$ where $\\mathbf{l} = (1, 1, \\dots, 1)^{\\mathsf{T}}$, and $\\mu_i$ is the expected return on asset $i$ and $x_i$ is the proportion of wealth invested in each asset $i$, for $i = 1, 2, \\dots, n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f93607d-dea2-4335-a309-308f7abf2292",
   "metadata": {},
   "source": [
    "Minimizing $-\\boldsymbol{\\mu}^{\\mathsf{T}}\\mathbf{x}$ is equivalent to maximizing $\\boldsymbol{\\mu}^{\\mathsf{T}}\\mathbf{x}$ so the given problem is equivalent to maximizing the total expected return $\\mu_1 x_1 + \\cdots + \\mu_n x_n$ subject to a **no short sales constraint** $(\\mathbf{x} \\geq 0)$ and the budget constraint which requires the sum of the proportions to be 1. \n",
    "\n",
    "Notice that,\n",
    "- The budget constraint is *always active*,\n",
    "- $n - 1$ of he non-negativity constraints must be active at any extreme point,\n",
    "- the extreme points are those at which 100% is invested in any given asset.\n",
    "\n",
    "By the above theorem, one of the extreme points is an optimal solution, so the objective function at these points are just $-\\mu_1, \\dots, \\mu_n$ and the smallest of these occurs for that index $k$ at which $\\mu_k = \\max\\{\\mu_1, \\dots, \\mu_n\\}$ and the optimal holdings is $\\mathbf{x}^\\ast$ where $x^\\ast_i = 0$ for $i = 1, \\dots, n$, $i \\neq k$, and $x_k^\\ast = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef04d9-768d-4421-8c57-134889d70ddf",
   "metadata": {},
   "source": [
    "$\\star$ If one is trying to maximize his/her expected return subject to no short sales constraints, the optimal solution is to invest all wealth into the asset with the highest expected return. We also have the Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "014f854f-bf86-4aac-9174-b28e65a07c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution x = [1. 1.]\n",
      "Multipliers for Ax = b: [2.]\n"
     ]
    }
   ],
   "source": [
    "%run quadratic_minimization_script.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
