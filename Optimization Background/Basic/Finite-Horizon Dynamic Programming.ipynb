{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da22450-f81a-4dbc-a71a-fc26a9d5eb64",
   "metadata": {},
   "source": [
    "# Finite-Horizon Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e181a-8917-49fc-ac77-6fa9de0b7cae",
   "metadata": {},
   "source": [
    "- A **dynamic programming problem** is an optimization problem in which decisions have to be taken sequentially over several time periods.\n",
    "- It is usually assumed that the periods are \"linked\" viz. that actions taken in any particular period affect the decision environment (and reward possibilities) in all future periods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506d0a4-17bf-43ee-9b44-1035a3114129",
   "metadata": {},
   "source": [
    "### 11.1 - Finite-Horizon Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24d521-d7d2-4ed1-90aa-e136efed0c99",
   "metadata": {},
   "source": [
    "A **Finite Horizon (Markovian) Dynamic Programming Problem** (FHDP) is defined by a tuple $\\{S, A, T, (r_t, f_t, \\Phi_t)_{t = 1}^T\\}$ where\n",
    "1. $S$ is the **state space** of the problem, with generic element $s$,\n",
    "2. $A$ is the **action space** of the problem, with generic element $a$,\n",
    "3. $T$, a positive integer, is the **horizon** of the problem,\n",
    "4. For each $t \\in \\{1, \\dots, T\\}$,\n",
    "   - $r_t : S \\times A \\to \\mathbb{R}$ is the period-$t$ **reward function**,\n",
    "   - $f_t: S \\times A \\to S$ is the period-$t$ **transition function**, and\n",
    "   - $\\Phi_t: S \\to P(A)$ is the **feasible action correspondence**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3fa879-47bb-49d3-aa24-ddc52567639e",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Decision-maker begins from some initial state $s_1 = s \\in S$. The set of actions available to the decision maker at this state is given by the correspondence $\\Phi_1(s_1) \\subset A$.\n",
    "- When the decision-maker choses an action $a_1 \\in \\Phi_1(s)$, two things happen:\n",
    "  - First, the decision-maker receives an immediate reward of $r_1(s_1, a_1)$.\n",
    "  - Second, the state $s_2$ at the beginning of period 2 as realized as $s_2 = f_1(s_1, a_1)$. At this new state, the set of feasible actions is now given by $\\Phi_2(s_2) \\subset A$.\n",
    "- When an action $a_2 \\in \\Phi_2(s_2)$ is chosen, a reward $r_2(s_2, a_2)$ is recieved, and the period-3 state $s_3$ is realized as $s_3 = f_2(s_2, a_2)$, and so on until the terminal date $T$.\n",
    "\n",
    "**Objective**:\n",
    "- Choose a plan for taking actions at each point in time in order to maximize the sum of per-period rewards over the horizon of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a225353-7a72-4614-b9eb-72f0c2fdcb0b",
   "metadata": {},
   "source": [
    "I.e., we want to solve $$\\text{Maximize } \\sum_{t = 1}^T r_t(s_t, a_t) \\qquad \\text{subject to } \\begin{cases} s_1 = s \\in S, \\\\ s_t = f_{t - 1}(s_{t - 1}, a_{t - 1}), \\qquad t = 2, \\dots, T, \\\\ a_t \\in \\Phi_t(s_t), \\qquad \\qquad\\qquad   t = 1, \\dots, T.\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e445e873-8aff-4028-9a0e-75e811bc41b6",
   "metadata": {},
   "source": [
    "### 11.2 - Histories, Strategies, and the Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592cdbdd-70ff-4d5e-9876-0425955ea875",
   "metadata": {},
   "source": [
    "- A *t*-history $\\eta_t$ is a vector $\\{s_1, a_1, \\dots, s_{t - 1}, a_{t - 1}, s_t\\}$ of the state $s_\\tau$ in each period $\\tau$ up to $t$, the action $a_\\tau$ taken that period, and the period-$t$ time state $s_t$.\n",
    "- Let $H_1 = S$, and for $t > 1$, let $H_t$ denote the set of all possible $t$-histories $\\eta_t$.\n",
    "- Given a $t$-history $\\eta_t$, $s_t[\\eta_t]$ denotes the period-$t$ state under the history $\\eta_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70f9ad-cccb-4fda-ade3-8aaf37d69be0",
   "metadata": {},
   "source": [
    "- A **strategy** $\\sigma$ for the problem is a seequence $\\{\\sigma_t\\}_{t = 1}^T$ where for each $t$, $\\sigma_t : H_t \\to A$ specifies the action $\\sigma_t(\\eta_t) \\in \\Phi_t(s_t[\\eta_t])$ to be taken in period $t$ as a function of the history $\\eta_t \\in H_t$ up to $t$.\n",
    "- The requirement $\\sigma_t(\\eta_t) \\in \\Phi_t(s_t[\\eta_t])$ ensures that the feasibility of actions at all points is built into the definition of a strategy.\n",
    "- $\\Sigma$ denotes the set of all strategies $\\sigma$ for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f88744a-a62c-481a-b8c2-106d4c9dbd8c",
   "metadata": {},
   "source": [
    "Each strategy $\\sigma \\in \\Sigma$ gives rise from each initial state $s \\in S$ to a **unique sequence of states and actions** $\\{s_t(\\sigma, s), a_t(\\sigma, s)\\}$, and therefore, to a unique sequence of histories $\\eta_t(\\sigma, s)$. In the obvious recursive manner, we have $s_1(\\sigma, s) = 1$, and for $t = 1, \\dots, T$, \n",
    "$$\\begin{align*} \\eta_t(\\sigma, s) &= \\{s_1(\\sigma, s), a_1(\\sigma, s), \\dots, s_t(\\sigma, s)\\} \\\\ a_t(\\sigma, s) &= \\sigma_t[\\eta_t(\\sigma, s)] \\\\ s_{t + 1}(\\sigma, s) &= f_t[s_t(\\sigma, s), a_t(\\sigma, s)].\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d5378-7542-4719-8e7e-076bada5313b",
   "metadata": {},
   "source": [
    "- Given an initial state $s_1 = s \\in S$, each strategy $\\sigma$ gives rise to a unique period-$t$ reward $r_t(\\sigma)(s)$ from $s$ defined as $$r_t(\\sigma)(s) = r_t[s_t(\\sigma, s), a_t(\\sigma, s)].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2279a7e-1f04-481a-99e9-d82f5048827a",
   "metadata": {},
   "source": [
    "- The **total reward** under $\\sigma$ from the initial state $s$, denoted $W(\\sigma)(s)$ is $$W(\\sigma)(s) = \\sum_{t = 1}^T r_t(\\sigma)(s).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35013c-305e-4a01-9802-77cccdf224e7",
   "metadata": {},
   "source": [
    "- The **value function** $V: S \\to \\mathbb{R}$ of the problem is defined by $$V(s) = \\sup_{\\sigma \\in \\Sigma} W(\\sigma)(s).$$\n",
    "- The strategy $\\sigma^\\ast$ is **optimal** if the payoff it generates from any initial state is the supremum of all possible payoffs from that state. I.e., if $$W(\\sigma^\\ast)(s) = V(s), \\qquad (\\forall s \\in S).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f6dfb6-8b46-4758-ba72-1235df776981",
   "metadata": {},
   "source": [
    "### 11.3 - Markovian Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb100b-a203-4339-803a-d8c64768b348",
   "metadata": {},
   "source": [
    "A **Markovian strategy** is a strategy $\\sigma$ in which at each $t = 1, \\dots, T - 1$, $\\sigma_t$ depends on the $t$-history $\\eta_t$ only through $t$ and the value of the period-$t$ state $s_t[\\eta_t]$ under $\\eta_t$. \n",
    "- Such a strategy can be represented by a sequence $\\{g_1, \\dots, g_T\\}$ where for each $t$, $g_t: S \\to A$ specifies the action $g_t(s_t) \\in \\Phi_t(s_t)$ to be taken in period $t$, as a function of only the period-$t$ state $s_t$.\n",
    "- If a Markovian strategy is optimal, it is called a **Markovian optimal strategy** (optimal in the class of all strategies, not just Markovian ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd349775-4e84-4337-8700-045162dd48ba",
   "metadata": {},
   "source": [
    "**Lemma (Backwards Induction for FHDP):** Let $\\sigma = (\\sigma_1, \\dots, \\sigma_T)$ be an optimal strategy for the FHDP $$\\{S, A, T, (r_t, f_t, \\Phi_t)_{t = 1}^T\\}.$$ Suppose that for some $\\tau \\in \\{1, \\dots, T\\}$, the $(T - \\tau + 1)$-period continuation problem $$\\{S, A, T - \\tau + 1, (r_t, f_t, \\Phi_t)_{t = \\tau}^T\\}$$ admits a Markovian optimal strategy $\\{g_\\tau, \\dots, g_T\\}$. Then, the strategy $$\\{\\sigma_1, \\dots, \\sigma_{\\tau - 1}, g_\\tau, \\dots, g_T\\}$$ is an optimal strategy for the original problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349a07c-3c85-4a27-9e2a-6cec78b960b4",
   "metadata": {},
   "source": [
    "**Backwards Induction Process:** \n",
    "- Consider the one-period problem in which, for any $s \\in S$, we solve $$\\text{Maximize } r_T(s, a) \\text{ subject to } a \\in \\Phi_T(s).$$\n",
    "- Let $g_T^\\ast(s)$ be a solution to this problem at the state $s$.\n",
    "- By the above lemma, the solution $g_T^\\ast$ can be used in the last period of an optimal strategy, without changing the total rewards.\n",
    "- Now, **given** that we are going to use $g_T^\\ast$ in the last period, we can find the actions in the two-period problem beginning at period-$(T - 1)$ that will be optimal for the two-period problem.\n",
    "- We obtain the strategy pair $(g_{T - 1}^\\ast, g_T)$ that is optimal for the two-period continuation problem beginning at period $T - 1$.\n",
    "- Continuing inductively, we can construct the optimal strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede0720-6ebc-433c-937e-df9374c7fae1",
   "metadata": {},
   "source": [
    "**Theorem (The Bellman Equation)**: Suppose that under the FHDP:\n",
    "1. For each $t$, $r_t$ is continuous and bounded on $S \\times A$,\n",
    "2. For each $t$, $f_t$ is continuous on $S \\times A$, and\n",
    "3. For each $t$, $\\Phi_t$ is a continuous, comapct-valued correspondence on $S$.\n",
    "Then, the FHDP admits a Markovian optimal strategy. The value function $V_t(\\cdot)$ of the $(T - t + 1)$-period continuation problem statisfies for each $t \\in \\{1, \\dots, T\\}$ and $s \\in S$, the following condition called the **Bellman equation** or the **Bellman principle of optimality**: $$V_t(s) = \\max_{a \\in \\Phi_t(s)} \\{r_t(s, a) + V_{t + 1}[f_t(s,a)]\\}.$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
