{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da22450-f81a-4dbc-a71a-fc26a9d5eb64",
   "metadata": {},
   "source": [
    "# Finite-Horizon Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e181a-8917-49fc-ac77-6fa9de0b7cae",
   "metadata": {},
   "source": [
    "- A **dynamic programming problem** is an optimization problem in which decisions have to be taken sequentially over several time periods.\n",
    "- It is usually assumed that the periods are \"linked\" viz. that actions taken in any particular period affect the decision environment (and reward possibilities) in all future periods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506d0a4-17bf-43ee-9b44-1035a3114129",
   "metadata": {},
   "source": [
    "### 11.1 - Finite-Horizon Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24d521-d7d2-4ed1-90aa-e136efed0c99",
   "metadata": {},
   "source": [
    "A **Finite Horizon (Markovian) Dynamic Programming Problem** (FHDP) is defined by a tuple $\\{S, A, T, (r_t, f_t, \\Phi_t)_{t = 1}^T\\}$ where\n",
    "1. $S$ is the **state space** of the problem, with generic element $s$,\n",
    "2. $A$ is the **action space** of the problem, with generic element $a$,\n",
    "3. $T$, a positive integer, is the **horizon** of the problem,\n",
    "4. For each $t \\in \\{1, \\dots, T\\}$,\n",
    "   - $r_t : S \\times A \\to \\mathbb{R}$ is the period-$t$ **reward function**,\n",
    "   - $f_t: S \\times A \\to S$ is the period-$t$ **transition function**, and\n",
    "   - $\\Phi_t: S \\to P(A)$ is the **feasible action correspondence**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3fa879-47bb-49d3-aa24-ddc52567639e",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Decision-maker begins from some initial state $s_1 = s \\in S$. The set of actions available to the decision maker at this state is given by the correspondence $\\Phi_1(s_1) \\subset A$.\n",
    "- When the decision-maker choses an action $a_1 \\in \\Phi_1(s)$, two things happen:\n",
    "  - First, the decision-maker receives an immediate reward of $r_1(s_1, a_1)$.\n",
    "  - Second, the state $s_2$ at the beginning of period 2 as realized as $s_2 = f_1(s_1, a_1)$. At this new state, the set of feasible actions is now given by $\\Phi_2(s_2) \\subset A$.\n",
    "- When an action $a_2 \\in \\Phi_2(s_2)$ is chosen, a reward $r_2(s_2, a_2)$ is recieved, and the period-3 state $s_3$ is realized as $s_3 = f_2(s_2, a_2)$, and so on until the terminal date $T$.\n",
    "\n",
    "**Objective**:\n",
    "- Choose a plan for taking actions at each point in time in order to maximize the sum of per-period rewards over the horizon of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a225353-7a72-4614-b9eb-72f0c2fdcb0b",
   "metadata": {},
   "source": [
    "I.e., we want to solve $$\\text{Maximize } \\sum_{t = 1}^T r_t(s_t, a_t) \\qquad \\text{subject to } \\begin{cases} s_1 = s \\in S, \\\\ s_t = f_{t - 1}(s_{t - 1}, a_{t - 1}), \\qquad t = 2, \\dots, T, \\\\ a_t \\in \\Phi_t(s_t), \\qquad \\qquad\\qquad   t = 1, \\dots, T.\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ee8c8-db48-4f2f-a9ce-75dd4974939d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
